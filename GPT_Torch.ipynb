{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYM1Bk1YyqK3",
        "outputId": "478b1a64-c25f-457a-ddfb-c2eb005315ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Would you like to load the model[y|n]?n\n",
            "! dove-feather lining churchmen terrible wash show crafty nero lewd-tongued hector hen madmen trial stands sheathing tied-up freckled thither besides modesties late berries wrenching walked nine revenge near'st enough hopeful auger guides stale melancholy part jack shrill-voiced pounds hate soon-believing courtier strain fare groaning dimm conqueror glose clears hallowed sphere bastards northumberland pilgrimage goodlier pies trees quote started parcell deer politic mistrustful pointing bots sinks tomorrow adder looking-glass vincentio family suffrage ivory promise-keeping slack foreward mockery freckled avails temptations zeal danced ears snip smilingly coal-black orb grieving schoolboys gracious limps least pleasant argue table-book chisel horizon pretty me tamed gilded scratching\n",
            "! untangled o'erpast festival displeased lazy-pacing , stone-hard bled asses septentrion thither patent night-crow taken unattainted sufficient thou'ldst wholesome yew injured haven paltering lust imposed speaketh shrugs stalls miscarries chop-logic takes negligence dates causes varlets extol dry-beat kites swearing attorneys-general determination scaled dispute fled consenting permitted lawn declined frightful hests room\n",
            "Epoch: 0, Loss: 9.583723068237305, Perplexity: 14526.401534275717\n",
            "! for indeed , her flexible , to what wouldst whom pity , and spend and that done , whose parents , then i have made the awful shoes , for such decay made buried ; and most grief than right . camillo too ill lady anne , for a goodly\n",
            "Epoch: 300, Loss: 4.865359306335449, Perplexity: 129.71753857290517\n",
            "! capulet : stay . he is it is at our cook word that to strange lunacy ? capulet : are beguiled and with him a tide ; catched . capulet : the heir will pass . gloucester : stay , warwick : it not i slew me , brave men\n",
            "Epoch: 600, Loss: 4.342032432556152, Perplexity: 76.86360077700216\n",
            "! you get your son 's meet you buy lads to him : harry good prepare us , and to bed ? must die ' and say you , drybeat all together with't , sir : peace ? nurse : good den , i loved me this , a corse ;\n",
            "Epoch: 900, Loss: 3.965480327606201, Perplexity: 52.74559847308355\n",
            "! a wench , but not wounded ? dark corners the pox , was the dish , by the child ; and be so to doom it 's eye , and hanging , nought , not in a beauty rarer by their opposite to express women ? or the nature ,\n",
            "Epoch: 1200, Loss: 3.632425546646118, Perplexity: 37.804401840256794\n",
            "! a man 's wings . claudio : my good man 's some other letter ; but this apparel fits another thing to much upon you be the other grow , doth ride , if you shall write to reside . mariana : i am as i am about to my\n",
            "Epoch: 1500, Loss: 3.3402490615844727, Perplexity: 28.226155881130783\n",
            "! and , do as glorious for these are here ? gentle tyrrel : what , why dost thou wilt thou didst discords ? ? let me speak richard : for courteous , in thy canopy : and what with him ? messenger : upon thyself do mercy ? king richard\n",
            "Epoch: 1800, Loss: 3.0221304893493652, Perplexity: 20.534994700755902\n",
            "! would they that did weep , in want their lies that breathe from my true beauty needs must needs shed remorseful tear ; nor give me but to let me embrace thee courtesy . my brother : hast thou slay ; therefore , yet put on the child , when\n",
            "Epoch: 2100, Loss: 2.880338191986084, Perplexity: 17.82029884289754\n",
            "! king richard iii : farewell , in good what thinkest thou else ? ratcliff : good boy . ratcliff : peace conduct us with you , my lord . ratcliff , thou shalt to show the good soul , and go'st foremost ; but what is dispersed and thou hast\n",
            "Epoch: 2400, Loss: 2.6690027713775635, Perplexity: 14.425576419852048\n",
            "! foremost . benvolio : my good mercutio : tybalt : romeo , come , switch and mercutio , take my pump the capulets . mercutio : how much fair assembly ? mercutio : mercutio : my pump ! mercutio : my man ! mercutio : you rat-catcher , good den\n",
            "Epoch: 2700, Loss: 2.3762688636779785, Perplexity: 10.764663414517745\n",
            "! what news ? we 'll tell him everlastingly no time serve , nor thought of such faults upon him . angelo : the fliers at what i 'll make trial . isabella : we are too to attempt you : good our brother come to do't , intends you .\n",
            "Epoch: 3000, Loss: 2.3167777061462402, Perplexity: 10.142938064519504\n",
            "! polixenes : he shall appear . perdita : polixenes for my lord . florizel : thou fond woman ? i have a true in bohemia . camillo : but one of this , its servant : he met , on thee offer 'd time to make a stubborn soul remembering\n",
            "Epoch: 3300, Loss: 2.1733553409576416, Perplexity: 8.787720426840355\n",
            "! tranio : why , she sings open 'd so ; had i , tell her nine , and her once : god , if she chide her kindred weep ; and all , the poor babe , god we are met of blessed land would , you : for your\n",
            "Epoch: 3600, Loss: 2.1038544178009033, Perplexity: 8.19770648175701\n",
            "! capulet : my lord , i can not send to tell you and tell me the world man ; -- hold up all are marvellous proper man ! fly , hated you look 'd , the county paris : bear the rest is come ? capulet : she hangs about\n",
            "Epoch: 3900, Loss: 2.0652496814727783, Perplexity: 7.88726695625578\n",
            "! there in gold would i 'll bury thee but to thee , if thou dost kill a poison , jule , or live to thee and in thee an some death , or lose mine . hortensio : signior baptista , and you a husband take it ? petruchio :\n",
            "Epoch: 4200, Loss: 1.9259538650512695, Perplexity: 6.861690672232414\n",
            "! if she be a week ; she can persuade the encounter men . leontes : o ' the gods ! the night 's not , i 'll swear't . shepherd : i have perused the queen , if you 'll stay , to her lest you , pray you tyrant\n",
            "Epoch: 4500, Loss: 1.7875713109970093, Perplexity: 5.974923599235586\n",
            "! i am well for love be so that so fair suspicion ! and three-and-twenty , which and so may have power i may angelo : yet are so to think so to make her and my lucio hermione ; so prove true , in me with nothing less shows good\n",
            "Epoch: 4800, Loss: 1.7375861406326294, Perplexity: 5.683607421938715\n",
            "! york : alack are too quick and bring content . i am agreed , that yet you are weary of victory . king richard ii : i would learn it , good fools ; yet am i sent upon me the word . king is not ; and by that is usurp . cousin scroop : go thither gone to us and take our countrymen , love my heart i 'll fight . you shall we have a slave , the way to slay the way . set them , or , a short , lords , my sovereign , and i unto the fully in the clime of you ; come to let 's castle : what is it , that he shall be found , or will prove to use me rogue . bishop of york : content thee to him . what , cousin , brother , is i love him in fight with all that means to use . come , clifford : bring him hope . warwick : what noise and not , at my father , shall make an george : it go , dost thou shouldst please ; and leave the crown : york , and servant in all at all the rest . king of aumerle : welcome , even wherein thou turn me all 's , and not : exton . northumberland : ay , cousin of york : now northumberland , shall i be forgot ; make them . king henry bolingbroke : if i have , i will i be patient . queen margaret : fearful bastard , gentle uncle , of you , shall feel the king , you be it . beat me so ; i dead , my state nor any kingdom lost . keeper : it is too , and berkeley , noble york : it is . edward : 'tis false . york : o king richard miss thy father , sweet prince of gloucester , give me some charge , if 'twere neither me leave . what , and blunt : my brother , that i did ; thy tongue that thou wouldst have murdered where one take my head ere i 'll take my hand , my leave of our country 's point . king richard ii : o , make me not thy brother , and dangerous cousin , and thou didst love me but not lived ? who is my heart ? queen as full of thy son , my love ? king richard ii : then began , if grace to seek him name be my heart . that thou wouldst be wrongs i thee so much : well , too noble cousin . but thou not for an executioner . king richard ii : hold me , be plain , so that i was nor any kingdom : yes , even here i was a king , my father , so : o , but knows me not . king of that by that great york 's friend , thou canst give . murder thy husband 's friend , who loved ? but stay now i may : i never show when thou wert thou not now i do as an enemy to thee to kill thy son 's ground , my breast , unless my tongue . is back to shame . duke of aumerle : o happy wife ! why , what news ? so doth i will i be ? queen margaret : get thee hence , thou not kill thee hence . gloucester : no friend of thy son thou canst thou shouldst please . i have any word : thy name ; lest that i be thyself ; and will turn to make thee . gloucester : i forget , thou didst kill your husband , by deep , and all alone . lady anne : how ! gloucester : thy father , i know that murder thee do repent me . queen margaret : so thou kill me . gloucester : wert thou unfit for duty , but now thou unfit for this deed . lady anne : i must either be patient ere didst kill my heart . lady anne , to me . gloucester : king edward iv : by my children sense me , now that is done so by . gloucester , of gloucester , or me a man , i 'll light back a thousand , i awhile back for him : and if thou hadst a king , as thou hadst an executioner . lady capulet : now to be , come , madam , by my sword . king edward , you are not hence ? lady grey : to me , by love i am more . gloucester : how shall you be a word ? lady grey : the death . yes , by change of our father laid to my heart . lady bona , my niece ; alas poor soul , stay awhile . thus i 'll play the corse : a happy time alone , girl . prince edward : i 'll not endure the dog of my tyranny above our house ! gloucester : i thomas for if any call them . lady anne : urge their lives , of my gracious lord , when i 'll bear once . lady anne : i 'll be thy death , no doubt not yet i am loath to kill thee . gloucester : 'tis very patient in grief ; for 't is no , for that i need of thine . thou hast slaughter 'd , for the prince of gloucester : but now it was ever he was my lord . lady anne : i am no more than the man that was . lady anne : and , he is too fond to 't was thou wert the truth to me ; and i give him alone .\n",
            "Would you like to save the model[y|n]?y\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "block_size = 256\n",
        "learning_rate = 9e-4\n",
        "eval_interval = 300 # Every n step, we do an evaluation.\n",
        "iterations = 5000 # Like epochs\n",
        "eval_iters = 100\n",
        "batch_size = 64\n",
        "embeds_size = 195\n",
        "num_heads = 5\n",
        "num_layers = 5\n",
        "drop_prob = 0.15\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "with open('shakespeare.txt') as fp:\n",
        "    text = fp.read()\n",
        "words = word_tokenize(text.lower())  # Tokenize into words\n",
        "vocab = sorted(set(words))\n",
        "vocab_size = len(vocab)\n",
        "stoi = {s: i for i, s in enumerate(vocab)}\n",
        "itos = {i: s for s, i in stoi.items()}\n",
        "encode = lambda s: [stoi[w] for w in word_tokenize(s.lower())]\n",
        "decode = lambda e: ' '.join([itos[x] for x in e])\n",
        "\n",
        "# Data preparation\n",
        "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
        "train_split = int(0.9 * len(data))\n",
        "train_data = data[:train_split]\n",
        "test_data = data[train_split:]\n",
        "\n",
        "def calculate_perplexity(loss, num_predictions):\n",
        "    return math.exp(loss)\n",
        "def get_batch(split='train', block_size=block_size, batch_size=batch_size):\n",
        "\t'''\n",
        "\t\tCreate a random batch and returning batch along with targets.\n",
        "\t'''\n",
        "\tdata = train_data if split == 'train' else test_data\n",
        "\tix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\tx = torch.stack([data[i:i + block_size] for i in ix])\n",
        "\ty = torch.stack([data[i+1:i + block_size + 1] for i in ix])\n",
        "\treturn x, y\n",
        "\n",
        "class head(nn.Module):\n",
        "\t'''\n",
        "\t\tCommunication between tokens happen here.\n",
        "\t'''\n",
        "\tdef __init__(self, head_size):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.key = nn.Linear(embeds_size, head_size, bias=False)\n",
        "\t\tself.query = nn.Linear(embeds_size, head_size, bias=False)\n",
        "\t\tself.value = nn.Linear(embeds_size, head_size, bias=False)\n",
        "\t\tself.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\t\tself.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tB,T,C = x.shape\n",
        "\t\t# What am I looking for?\n",
        "\t\tq = self.query(x)\n",
        "\t\t# What do I have?\n",
        "\t\tk = self.key(x)\n",
        "\t\t# What is the representation value of me?\n",
        "\t\t# Or: what's my personality in a group?\n",
        "\t\t# Or: what mask do I have when I'm in a group?\n",
        "\t\tv = self.value(x)\n",
        "\t\tscores = q @ k.transpose(-2, -1) * (1 / math.sqrt(C)) # (B,T,head_size) @ (B,head_size,T) --> (B,T,T)\n",
        "\t\tscores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\t\tscores = F.softmax(scores, dim=-1)\n",
        "\t\tscores = self.dropout(scores)\n",
        "\t\tout = scores @ v\n",
        "\t\treturn out\n",
        "\n",
        "\n",
        "class multihead(nn.Module):\n",
        "\t'''\n",
        "\t\tI have multiple personalities(v), tendencies and needs (q), and valuable things (k) in different groups.\n",
        "\t'''\n",
        "\tdef __init__(self, num_heads, head_size):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.multihead = nn.ModuleList([head(head_size) for _ in range(num_heads)])\n",
        "\t\tself.output_linear = nn.Linear(embeds_size, embeds_size)\n",
        "\t\tself.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "\tdef forward(self, hidden_state):\n",
        "\t\thidden_state = torch.cat([head(hidden_state) for head in self.multihead], dim=-1)\n",
        "\t\thidden_state = self.output_linear(hidden_state)\n",
        "\t\thidden_state = self.dropout(hidden_state)\n",
        "\t\treturn hidden_state\n",
        "\n",
        "\n",
        "class transformer_block(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\thead_size = embeds_size // num_heads\n",
        "\t\tself.n_heads = multihead(num_heads, head_size)\n",
        "\t\tself.ffn = nn.Sequential(\n",
        "\t\t\tnn.Linear(embeds_size, 4 * embeds_size),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(4 * embeds_size, embeds_size),\n",
        "\t\t\tnn.Dropout(drop_prob),\n",
        "\t\t)\n",
        "\t\tself.ln1 = nn.LayerNorm(embeds_size)\n",
        "\t\tself.ln2 = nn.LayerNorm(embeds_size)\n",
        "\n",
        "\tdef forward(self, hidden_state):\n",
        "\t\thidden_state = hidden_state + self.n_heads(self.ln1(hidden_state))\n",
        "\t\thidden_state = hidden_state + self.ffn(self.ln2(hidden_state))\n",
        "\t\treturn hidden_state\n",
        "\n",
        "\n",
        "# We do feed-forward n times where n is block_size\n",
        "class transformer(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.stack = nn.ModuleDict(dict(\n",
        "\t\t\ttok_embs=nn.Embedding(vocab_size, embeds_size),\n",
        "\t\t\tpos_embs=nn.Embedding(block_size, embeds_size),\n",
        "\t\t\tdropout=nn.Dropout(drop_prob),\n",
        "\t\t\tblocks=nn.Sequential(\n",
        "\t\t\t\ttransformer_block(),\n",
        "\t\t\t\ttransformer_block(),\n",
        "\t\t\t\ttransformer_block(),\n",
        "\t\t\t\ttransformer_block(),\n",
        "\t\t\t\ttransformer_block(),\n",
        "\t\t\t),\n",
        "\t\t\tln=nn.LayerNorm(embeds_size),\n",
        "\t\t\tlm_head=nn.Linear(embeds_size, vocab_size),\n",
        "\t\t))\n",
        "\n",
        "\tdef forward(self, seq, targets=None):\n",
        "\t\tB, T = seq.shape\n",
        "\t\ttok_emb = self.stack.tok_embs(seq) # (batch, block_size, embed_dim) (B,T,C)\n",
        "\t\tpos_emb = self.stack.pos_embs(torch.arange(T, device=device))\n",
        "\t\tx = tok_emb + pos_emb\n",
        "\t\tx = self.stack.dropout(x)\n",
        "\t\tx = self.stack.blocks(x)\n",
        "\t\tx = self.stack.ln(x)\n",
        "\t\tlogits = self.stack.lm_head(x) # (B, block_size, vocab_size)\n",
        "\n",
        "\t\tif targets is None:\n",
        "\t\t\tloss = None\n",
        "\t\telse:\n",
        "\t\t\tB, T, C = logits.shape\n",
        "\t\t\tlogits = logits.view(B * T, C)\n",
        "\t\t\ttargets = targets.view(B * T)\n",
        "\t\t\tloss = F.cross_entropy(logits, targets)\n",
        "\n",
        "\t\treturn logits, loss\n",
        "\n",
        "\tdef autocomplete(self, idx, _len=10):\n",
        "\t\tfor _ in range(_len):\n",
        "\t\t\tidx_cond = idx[:, -block_size:] # crop it\n",
        "\t\t\tlogits, _ = self(idx_cond)\n",
        "\t\t\tlogits = logits[:, -1, :] # we only care about the last probability\n",
        "\t\t\tprobs = F.softmax(logits, dim=-1)\n",
        "\t\t\t# It selects samples from probs. The higher the prob, the more the chance of being selected\n",
        "\t\t\tnext_idx = torch.multinomial(probs, num_samples=1) # (B, 1) one prediction for each batch\n",
        "\t\t\tidx = torch.cat((idx, next_idx), dim=1)\n",
        "\t\treturn idx\n",
        "\n",
        "\n",
        "model = transformer()\n",
        "if input('Would you like to load the model[y|n]?') == 'y':\n",
        "\tmodel.load_state_dict(torch.load('model.pth'))\n",
        "model.to(device)\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def generate(_len=100):\n",
        "\tsample = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\tgenerated = model.autocomplete(sample, _len)\n",
        "\tdecoded = decode(generated[0].tolist())\n",
        "\treturn decoded\n",
        "\n",
        "print(generate())\n",
        "\n",
        "for epoch in range(iterations):\n",
        "    X, y = get_batch()\n",
        "    pred, loss = model(X, y)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % eval_interval == 0:\n",
        "        model.eval()\n",
        "        print(generate(50))\n",
        "        perplexity = calculate_perplexity(loss.item(), batch_size * block_size)\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.item()}, Perplexity: {perplexity}\")\n",
        "        model.train()\n",
        "print(generate(1000))\n",
        "\n",
        "if input('Would you like to save the model[y|n]?') == 'y':\n",
        "\ttorch.save(model.state_dict(), 'model.pth')"
      ]
    }
  ]
}